{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operationalize end-to-end Amazon Personalize model deployment process using AWS Step Functions Data Science SDK\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Task-States](#Task-States)\n",
    "4. [Wait-States](#Wait-States)\n",
    "5. [Choice-States](#Choice-States)\n",
    "6. [Workflow](#Workflow)\n",
    "7. [Generate-Recommendations](#Generate-Recommendations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook describes using the AWS Step Functions Data Science SDK to create and manage an Amazon Personalize workflow. The Step Functions SDK is an open source library that allows data scientists to easily create and execute machine learning workflows using AWS Step Functions. For more information on Step Functions SDK, see the following.\n",
    "* [AWS Step Functions](https://aws.amazon.com/step-functions/)\n",
    "* [AWS Step Functions Developer Guide](https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html)\n",
    "* [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io)\n",
    "\n",
    "In this notebook we will use the SDK to create steps to create Personalize resources, link them together to create a workflow, and execute the workflow in AWS Step Functions. \n",
    "\n",
    "For more information, on Amazon Personalize see the following.\n",
    "\n",
    "* [Amazon Personalize](https://aws.amazon.com/personalize/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules from the SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stepfunctions\n",
      "  Downloading stepfunctions-1.1.2.tar.gz (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 6.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting sagemaker<2.0.0,>=1.71.0\n",
      "  Downloading sagemaker-1.72.1.tar.gz (299 kB)\n",
      "\u001b[K     |████████████████████████████████| 299 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: boto3>=1.9.213 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from stepfunctions) (1.16.19)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from stepfunctions) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker<2.0.0,>=1.71.0->stepfunctions) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker<2.0.0,>=1.71.0->stepfunctions) (3.11.4)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker<2.0.0,>=1.71.0->stepfunctions) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker<2.0.0,>=1.71.0->stepfunctions) (0.1.5)\n",
      "Collecting smdebug-rulesconfig==0.1.4\n",
      "  Downloading smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker<2.0.0,>=1.71.0->stepfunctions) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker<2.0.0,>=1.71.0->stepfunctions) (20.1)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9.213->stepfunctions) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9.213->stepfunctions) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.20.0,>=1.19.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9.213->stepfunctions) (1.19.19)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker<2.0.0,>=1.71.0->stepfunctions) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker<2.0.0,>=1.71.0->stepfunctions) (45.2.0.post20200210)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker<2.0.0,>=1.71.0->stepfunctions) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker<2.0.0,>=1.71.0->stepfunctions) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.19->boto3>=1.9.213->stepfunctions) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.19->boto3>=1.9.213->stepfunctions) (1.25.10)\n",
      "Building wheels for collected packages: stepfunctions, sagemaker\n",
      "  Building wheel for stepfunctions (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for stepfunctions: filename=stepfunctions-1.1.2-py2.py3-none-any.whl size=70532 sha256=ed6ae504033049bbdf8101af887707b6fb3764000e68e8e46a0c2c5dce0f127d\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/3b/83/5d/eb78e0ad08d5c70f915c4d11a19366b544790c3dcaa7bf3bfe\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-1.72.1-py2.py3-none-any.whl size=386572 sha256=f1411ce2bb532ab1cc8383455e06d4bd0439b71470bede74148aecbf58292838\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/65/21/9e/e0cfa0f2891e29445c2e241cd9eb4d9dbee64a0a8554f1cf7f\n",
      "Successfully built stepfunctions sagemaker\n",
      "Installing collected packages: smdebug-rulesconfig, sagemaker, stepfunctions\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 0.1.6\n",
      "    Uninstalling smdebug-rulesconfig-0.1.6:\n",
      "      Successfully uninstalled smdebug-rulesconfig-0.1.6\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.16.4.dev0\n",
      "    Uninstalling sagemaker-2.16.4.dev0:\n",
      "      Successfully uninstalled sagemaker-2.16.4.dev0\n",
      "Successfully installed sagemaker-1.72.1 smdebug-rulesconfig-0.1.4 stepfunctions-1.1.2\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade stepfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup IAM Roles\n",
    "\n",
    "#### Create an execution role for Step Functions\n",
    "\n",
    "You need an execution role so that you can create and execute workflows in Step Functions.\n",
    "\n",
    "1. Go to the [IAM console](https://console.aws.amazon.com/iam/)\n",
    "2. Select **Roles** and then **Create role**.\n",
    "3. Under **Choose the service that will use this role** select **Step Functions**\n",
    "4. Choose **Next** until you can enter a **Role name**\n",
    "5. Enter a name such as `StepFunctionsWorkflowExecutionRole` and then select **Create role**\n",
    "\n",
    "\n",
    "Attach a policy to the role you created. The following steps attach a policy that provides full access to Step Functions, however as a good practice you should only provide access to the resources you need.  \n",
    "\n",
    "1. Under the **Permissions** tab, click **Add inline policy**\n",
    "2. Enter the following in the **JSON** tab\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"personalize:*\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"lambda:InvokeFunction\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:PassRole\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"events:PutTargets\",\n",
    "                \"events:PutRule\",\n",
    "                \"events:DescribeRule\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "3. Choose **Review policy** and give the policy a name such as `StepFunctionsWorkflowExecutionPolicy`\n",
    "4. Choose **Create policy**. You will be redirected to the details page for the role.\n",
    "5. Copy the **Role ARN** at the top of the **Summary**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')\n",
    "\n",
    "\n",
    "import stepfunctions\n",
    "import logging\n",
    "\n",
    "from stepfunctions.steps import *\n",
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "workflow_execution_role = \"arn:aws:iam::820570838999:role/StepFunctionsWorkflowExecutionRole\" # paste the StepFunctionsWorkflowExecutionRole ARN from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup S3 location and filename\n",
    "create an Amazon S3 bucket to store the training dataset and provide the Amazon S3 bucket name and file name in the walkthrough notebook  step Setup S3 location and filename below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"comprehend-input123\"       # replace with the name of your S3 bucket\n",
    "filename = \"movie-lens\"  # replace with a name that you want to save the dataset under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_state_role = LambdaStep(\n",
    "    state_id=\"create bucket and role\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"stepfunction_create_personalize_role\", #replace with the name of the function you created\n",
    "        \"Payload\": {  \n",
    "           \"bucket\": bucket\n",
    "        }\n",
    "    },\n",
    "    result_path='$'\n",
    " \n",
    ")\n",
    "\n",
    "lambda_state_role.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=5,\n",
    "    max_attempts=1,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "lambda_state_role.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"CreateRoleTaskFailed\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach Policy to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1VFM9Z8QER2GASAW',\n",
       "  'HostId': 'AYeDSQGrsEDZO46vsXzcvAV6mWiapgMzGc62CtCIgeSfgO2OWlVMCwJj4Llagxpbbh/RAkIPcgg=',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'AYeDSQGrsEDZO46vsXzcvAV6mWiapgMzGc62CtCIgeSfgO2OWlVMCwJj4Llagxpbbh/RAkIPcgg=',\n",
       "   'x-amz-request-id': '1VFM9Z8QER2GASAW',\n",
       "   'date': 'Mon, 14 Dec 2020 20:02:22 GMT',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket)\n",
    "                \n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket, Policy=json.dumps(policy))\n",
    "\n",
    "# AmazonPersonalizeFullAccess provides access to any S3 bucket with a name that includes \"personalize\" or \"Personalize\" \n",
    "# if you would like to use a bucket with a different name, please consider creating and attaching a new policy\n",
    "# that provides read access to your bucket or attaching the AmazonS3ReadOnlyAccess policy to the role\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Personalize Role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = \"personalizeaccess\" # Create a personalize role\n",
    "\n",
    "\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"personalize.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName = role_name,\n",
    "    AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonPersonalizeFullAccess\"\n",
    "iam.attach_role_policy(\n",
    "    RoleName = role_name,\n",
    "    PolicyArn = policy_arn\n",
    ")\n",
    "\n",
    "time.sleep(60) # wait for a minute to allow IAM role policy attachment to propagate\n",
    "\n",
    "role_arn = create_role_response[\"Role\"][\"Arn\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download, Prepare, and Upload Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/personalize-data-science-sdk-workflow\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-14 20:06:36--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4924029 (4.7M) [application/zip]\n",
      "Saving to: ‘ml-100k.zip’\n",
      "\n",
      "ml-100k.zip         100%[===================>]   4.70M  4.67MB/s    in 1.0s    \n",
      "\n",
      "2020-12-14 20:06:37 (4.67 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
      "\n",
      "Archive:  ml-100k.zip\n",
      "   creating: ml-100k/\n",
      "  inflating: ml-100k/allbut.pl       \n",
      "  inflating: ml-100k/mku.sh          \n",
      "  inflating: ml-100k/README          \n",
      "  inflating: ml-100k/u.data          \n",
      "  inflating: ml-100k/u.genre         \n",
      "  inflating: ml-100k/u.info          \n",
      "  inflating: ml-100k/u.item          \n",
      "  inflating: ml-100k/u.occupation    \n",
      "  inflating: ml-100k/u.user          \n",
      "  inflating: ml-100k/u1.base         \n",
      "  inflating: ml-100k/u1.test         \n",
      "  inflating: ml-100k/u2.base         \n",
      "  inflating: ml-100k/u2.test         \n",
      "  inflating: ml-100k/u3.base         \n",
      "  inflating: ml-100k/u3.test         \n",
      "  inflating: ml-100k/u4.base         \n",
      "  inflating: ml-100k/u4.test         \n",
      "  inflating: ml-100k/u5.base         \n",
      "  inflating: ml-100k/u5.test         \n",
      "  inflating: ml-100k/ua.base         \n",
      "  inflating: ml-100k/ua.test         \n",
      "  inflating: ml-100k/ub.base         \n",
      "  inflating: ml-100k/ub.test         \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>RATING</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>13</td>\n",
       "      <td>225</td>\n",
       "      <td>2</td>\n",
       "      <td>882399156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>12</td>\n",
       "      <td>203</td>\n",
       "      <td>3</td>\n",
       "      <td>879959583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       USER_ID  ITEM_ID  RATING  TIMESTAMP\n",
       "0          196      242       3  881250949\n",
       "1          186      302       3  891717742\n",
       "...        ...      ...     ...        ...\n",
       "99998       13      225       2  882399156\n",
       "99999       12      203       3  879959583\n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget -N http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip -o ml-100k.zip\n",
    "data = pd.read_csv('./ml-100k/u.data', sep='\\t', names=['USER_ID', 'ITEM_ID', 'RATING', 'TIMESTAMP'])\n",
    "pd.set_option('display.max_rows', 5)\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['RATING'] > 2]                # keep only movies rated 2 and above\n",
    "data2 = data[['USER_ID', 'ITEM_ID', 'TIMESTAMP']] \n",
    "data2.to_csv(filename, index=False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(filename).upload_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda Task state\n",
    "\n",
    "A `Task` State in Step Functions represents a single unit of work performed by a workflow. Tasks can call Lambda functions and orchestrate other AWS services. See [AWS Service Integrations](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-service-integrations.html) in the *AWS Step Functions Developer Guide*.\n",
    "\n",
    "The following creates a [LambdaStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/compute.html#stepfunctions.steps.compute.LambdaStep) called `lambda_state`, and then configures the options to [Retry](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html#error-handling-retrying-after-an-error) if the Lambda function fails.\n",
    "\n",
    "#### Create a Lambda functions\n",
    "\n",
    "The Lambda task states in this workflow uses Lambda function **(Python 3.x)** that returns a Personalize resources such as Schema, Datasetgroup, Dataset, Solution, SolutionVersion, etc. Create the following functions in the [Lambda console](https://console.aws.amazon.com/lambda/).\n",
    "\n",
    "1. stepfunction-create-schema\n",
    "2. stepfunctioncreatedatagroup\n",
    "3. stepfunctioncreatedataset\n",
    "4. stepfunction-createdatasetimportjob\n",
    "5. stepfunction_select-recipe_create-solution\n",
    "6. stepfunction_create_solution_version\n",
    "7. stepfunction_getsolution_metric_create_campaign\n",
    "\n",
    "Copy/Paste the corresponding lambda function code from ./Lambda/ folder in the repo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Schema\n",
    "Before you add a dataset to Amazon Personalize, you must define a schema for that dataset. Once you define the schema and create the dataset, you can't make changes to the schema.for more information refer this documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_state_schema = LambdaStep(\n",
    "    state_id=\"create schema\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"stepfunction-create-schema\", #replace with the name of the function you created\n",
    "        \"Payload\": {  \n",
    "           \"input\": \"personalize-stepfunction-schema263\"\n",
    "        }\n",
    "    },\n",
    "    result_path='$'    \n",
    ")\n",
    "\n",
    "lambda_state_schema.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=5,\n",
    "    max_attempts=1,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "lambda_state_schema.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"CreateSchemaTaskFailed\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Datasetgroup\n",
    "Craete Datasetgroup: Creates an empty dataset group. A dataset group contains related datasets that supply data for training a model. A dataset group can contain at most three datasets, one for each type of dataset:\n",
    "•\tInteractions\n",
    "•\tItems\n",
    "•\tUsers\n",
    "To train a model (create a solution), a dataset group that contains an Interactions dataset is required. Call CreateDataset to add a dataset to the group.\n",
    "\n",
    "After you have created a schema , we will create another Stepfunction state based on this lambda function stepfunctioncreatedatagroup.py  below in github lambdas folder by running the Create Datasetgroup¶ step of the notebook. We are using python boto3 APIs to create_dataset_group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_state_datasetgroup = LambdaStep(\n",
    "    state_id=\"create dataset Group\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"stepfunctioncreatedatagroup\", #replace with the name of the function you created\n",
    "        \"Payload\": {  \n",
    "           \"input\": \"personalize-stepfunction-dataset-group\", \n",
    "           \"schemaArn.$\": '$.Payload.schemaArn'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    result_path='$'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "lambda_state_datasetgroup.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=5,\n",
    "    max_attempts=1,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "\n",
    "lambda_state_datasetgroup.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"CreateDataSetGroupTaskFailed\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset\n",
    "Creates an empty dataset and adds it to the specified dataset group. Use CreateDatasetImportJob to import your training data to a dataset.\n",
    "\n",
    "There are three types of datasets:\n",
    "\n",
    "Interactions\n",
    "\n",
    "Items\n",
    "\n",
    "Users\n",
    "\n",
    "Each dataset type has an associated schema with required field types. Only the Interactions dataset is required in order to train a model (also referred to as creating a solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_state_createdataset = LambdaStep(\n",
    "    state_id=\"create dataset\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"stepfunctioncreatedataset\", #replace with the name of the function you created\n",
    "#        \"Payload\": {  \n",
    "#           \"schemaArn.$\": '$.Payload.schemaArn',\n",
    "#           \"datasetGroupArn.$\": '$.Payload.datasetGroupArn',\n",
    "            \n",
    "            \n",
    "#        }\n",
    "        \n",
    "        \"Payload\": {  \n",
    "           \"schemaArn.$\": '$.schemaArn',\n",
    "           \"datasetGroupArn.$\": '$.datasetGroupArn',        \n",
    "        } \n",
    "        \n",
    "        \n",
    "    },\n",
    "    result_path = '$'\n",
    ")\n",
    "\n",
    "lambda_state_createdataset.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=5,\n",
    "    max_attempts=1,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "lambda_state_createdataset.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"CreateDataSetTaskFailed\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Import Job\n",
    "When you have completed Step 1: Creating a Dataset Group and Step 2: Creating a Dataset and a Schema, you are ready to import your training data into Amazon Personalize. When you import data, you can choose to import records in bulk, import records individually, or both, depending on your business requirements and the amount of historical data you have collected. If you have a large amount of historical records, \n",
    "we recommend you first import data in bulk and then add data incrementally as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_state_datasetimportjob = LambdaStep(\n",
    "    state_id=\"create dataset import job\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"stepfunction-createdatasetimportjob\", #replace with the name of the function you created\n",
    "        \"Payload\": {  \n",
    "           \"datasetimportjob\": \"stepfunction-createdatasetimportjob\",\n",
    "           \"dataset_arn.$\": '$.Payload.dataset_arn',\n",
    "           \"datasetGroupArn.$\": '$.Payload.datasetGroupArn',\n",
    "           \"bucket_name\": bucket,\n",
    "           \"file_name\": filename,\n",
    "           \"role_arn\": role_arn\n",
    "            \n",
    "        }\n",
    "    },\n",
    "\n",
    "    result_path = '$'\n",
    ")\n",
    "\n",
    "lambda_state_datasetimportjob.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=5,\n",
    "    max_attempts=1,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "lambda_state_datasetimportjob.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"DatasetImportJobTaskFailed\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Solution\n",
    "Once you have finished Preparing and Importing Data, you are ready to create a Solution. A Solution refers to the combination of an Amazon Personalize recipe, customized parameters, and one or more solution versions (trained models). Once you create a solution with a solution version, you can create a campaign to deploy the solution version and get recommendations.\n",
    "\n",
    "To create a solution in Amazon Personalize, you do the following:\n",
    "\n",
    "Choose a recipe – A recipe is an Amazon Personalize term specifying an appropriate algorithm to train for a given use case. See Step 1: Choosing a Recipe.\n",
    "\n",
    "Configure a solution – Customize solution parameters and recipe-specific hyperparameters so the model meets your specific business needs. See Step 2: Configuring a Solution.\n",
    "\n",
    "Create a solution version (train a model) – Train the machine learning model Amazon Personalize will use to generate recommendations for your customers. See Step 3: Creating a Solution Version.\n",
    "\n",
    "Evaluate the solution version – Use the metrics Amazon Personalize generates from the new solution version to evaluate the performance of the model. See Step 4: Evaluating the Solution Version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Recipe and Configuring a Solution\n",
    "A recipe is an Amazon Personalize term specifying an appropriate algorithm to train for a given use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_state_select_receipe_create_solution = LambdaStep(\n",
    "    state_id=\"select receipe and create solution\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"stepfunction_select-recipe_create-solution\", #replace with the name of the function you created\n",
    "        \"Payload\": {  \n",
    "           #\"dataset_group_arn.$\": '$.Payload.datasetGroupArn' \n",
    "            \"dataset_group_arn.$\": '$.datasetGroupArn'\n",
    "        }\n",
    "    },\n",
    "    result_path = '$'\n",
    ")\n",
    "\n",
    "lambda_state_select_receipe_create_solution.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=5,\n",
    "    max_attempts=1,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "lambda_state_select_receipe_create_solution.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"DatasetReceiptCreateSolutionTaskFailed\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Solution Version\n",
    "Once you have completed Choosing a Recipe and Configuring a Solution, you are ready to create a Solution Version.\n",
    "A Solution Version refers to a trained machine learning model you can deploy to get recommendations for customers. You can create a solution version using the console, AWS Command Line Interface (AWS CLI), or AWS SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_create_solution_version = LambdaStep(\n",
    "    state_id=\"create solution version\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"stepfunction_create_solution_version\", \n",
    "        \"Payload\": {  \n",
    "           \"solution_arn.$\": '$.Payload.solution_arn'           \n",
    "        }\n",
    "    },\n",
    "    result_path = '$'\n",
    ")\n",
    "\n",
    "lambda_create_solution_version.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=5,\n",
    "    max_attempts=1,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "lambda_create_solution_version.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"CreateSolutionVersionTaskFailed\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Campaign\n",
    "A campaign is used to make recommendations for your users. You create a campaign by deploying a solution version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_create_campaign = LambdaStep(\n",
    "    state_id=\"create campaign\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"stepfunction_getsolution_metric_create_campaign\", \n",
    "        \"Payload\": {  \n",
    "            #\"solution_version_arn.$\": '$.Payload.solution_version_arn'  \n",
    "            \"solution_version_arn.$\": '$.solution_version_arn'\n",
    "        }\n",
    "    },\n",
    "    result_path = '$'\n",
    ")\n",
    "\n",
    "lambda_create_campaign.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=5,\n",
    "    max_attempts=1,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "lambda_create_campaign.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"CreateCampaignTaskFailed\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait-States\n",
    "\n",
    "#### A `Wait` state in Step Functions waits a specific amount of time. See [Wait](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/states.html#stepfunctions.steps.states.Wait) in the AWS Step Functions Data Science SDK documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Schema to be ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_state_schema = Wait(\n",
    "    state_id=\"Wait for create schema - 5 secs\",\n",
    "    seconds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Datasetgroup to be ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_state_datasetgroup = Wait(\n",
    "    state_id=\"Wait for datasetgroup - 30 secs\",\n",
    "    seconds=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset to be ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_state_dataset = Wait(\n",
    "    state_id=\"wait for dataset - 30 secs\",\n",
    "    seconds=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Import Job to be ACTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_state_datasetimportjob = Wait(\n",
    "    state_id=\"Wait for datasetimportjob - 30 secs\",\n",
    "    seconds=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Receipe to ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_state_receipe = Wait(\n",
    "    state_id=\"Wait for receipe - 30 secs\",\n",
    "    seconds=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Solution Version to be ACTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_state_solutionversion = Wait(\n",
    "    state_id=\"Wait for solution version - 60 secs\",\n",
    "    seconds=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Campaign to be ACTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_state_campaign = Wait(\n",
    "    state_id=\"Wait for Campaign - 30 secs\",\n",
    "    seconds=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Check status of the lambda task and take action accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If a state fails, move it to `Fail` state. See [Fail](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/states.html#stepfunctions.steps.states.Fail) in the AWS Step Functions Data Science SDK documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check datasetgroup status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_state_datasetgroupstatus = LambdaStep(\n",
    "    state_id=\"check dataset Group status\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"stepfunction_waitforDatasetGroup\", #replace with the name of the function you created\n",
    "        \"Payload\": {  \n",
    "           \"input.$\": '$.Payload.datasetGroupArn',\n",
    "           \"schemaArn.$\": '$.Payload.schemaArn'\n",
    "        }\n",
    "    },\n",
    "    result_path = '$'\n",
    ")\n",
    "\n",
    "lambda_state_datasetgroupstatus.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=5,\n",
    "    max_attempts=1,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "lambda_state_datasetgroupstatus.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"DatasetGroupStatusTaskFailed\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check dataset import job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_state_datasetimportjob_status = LambdaStep(\n",
    "    state_id=\"check dataset import job status\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"stepfunction_waitfordatasetimportjob\", #replace with the name of the function you created\n",
    "        \"Payload\": {  \n",
    "           \"dataset_import_job_arn.$\": '$.Payload.dataset_import_job_arn',\n",
    "           \"datasetGroupArn.$\": '$.Payload.datasetGroupArn'\n",
    "        }\n",
    "    },\n",
    "    result_path = '$'\n",
    ")\n",
    "\n",
    "lambda_state_datasetimportjob_status.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=5,\n",
    "    max_attempts=1,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "lambda_state_datasetimportjob_status.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"DatasetImportJobStatusTaskFailed\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check solution version status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "solutionversion_succeed_state = Succeed(\n",
    "    state_id=\"The Solution Version ready?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_state_solutionversion_status = LambdaStep(\n",
    "    state_id=\"check solution version status\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"stepfunction_waitforSolutionVersion\", #replace with the name of the function you created\n",
    "        \"Payload\": {  \n",
    "           \"solution_version_arn.$\": '$.Payload.solution_version_arn'           \n",
    "        }\n",
    "    },\n",
    "    result_path = '$'\n",
    ")\n",
    "\n",
    "lambda_state_solutionversion_status.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=5,\n",
    "    max_attempts=1,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "lambda_state_solutionversion_status.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"SolutionVersionStatusTaskFailed\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check campaign status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_state_campaign_status = LambdaStep(\n",
    "    state_id=\"check campaign status\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"stepfunction_waitforCampaign\", #replace with the name of the function you created\n",
    "        \"Payload\": {  \n",
    "           \"campaign_arn.$\": '$.Payload.campaign_arn'           \n",
    "        }\n",
    "    },\n",
    "    result_path = '$'\n",
    ")\n",
    "\n",
    "lambda_state_campaign_status.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=5,\n",
    "    max_attempts=1,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "lambda_state_campaign_status.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"CampaignStatusTaskFailed\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice-States\n",
    "\n",
    "Now, attach branches to the Choice state you created earlier. See *Choice Rules* in the [AWS Step Functions Data Science SDK documentation](https://aws-step-functions-data-science-sdk.readthedocs.io) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain together steps for the define the workflow path\n",
    "\n",
    "The following cell links together the steps you've created above into a sequential group. The new path sequentially includes the Lambda state, Wait state, and the Succeed state that you created earlier.\n",
    "\n",
    "#### After chaining together the steps for the workflow path, we will define and visualize the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_campaign_choice_state = Choice(\n",
    "    state_id=\"Is the Campaign ready?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_campaign_choice_state.add_choice(\n",
    "    rule=ChoiceRule.StringEquals(variable=lambda_state_campaign_status.output()['Payload']['status'], value='ACTIVE'),\n",
    "    next_step=Succeed(\"CampaignCreatedSuccessfully\")     \n",
    ")\n",
    "create_campaign_choice_state.add_choice(\n",
    "    ChoiceRule.StringEquals(variable=lambda_state_campaign_status.output()['Payload']['status'], value='CREATE PENDING'),\n",
    "    next_step=wait_state_campaign\n",
    ")\n",
    "create_campaign_choice_state.add_choice(\n",
    "    ChoiceRule.StringEquals(variable=lambda_state_campaign_status.output()['Payload']['status'], value='CREATE IN_PROGRESS'),\n",
    "    next_step=wait_state_campaign\n",
    ")\n",
    "\n",
    "create_campaign_choice_state.default_choice(next_step=Fail(\"CreateCampaignFailed\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutionversion_choice_state = Choice(\n",
    "    state_id=\"Is the Solution Version ready?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutionversion_succeed_state = Succeed(\n",
    "    state_id=\"The Solution Version ready?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutionversion_choice_state.add_choice(\n",
    "    rule=ChoiceRule.StringEquals(variable=lambda_state_solutionversion_status.output()['Payload']['status'], value='ACTIVE'),\n",
    "    next_step=solutionversion_succeed_state   \n",
    ")\n",
    "solutionversion_choice_state.add_choice(\n",
    "    ChoiceRule.StringEquals(variable=lambda_state_solutionversion_status.output()['Payload']['status'], value='CREATE PENDING'),\n",
    "    next_step=wait_state_solutionversion\n",
    ")\n",
    "solutionversion_choice_state.add_choice(\n",
    "    ChoiceRule.StringEquals(variable=lambda_state_solutionversion_status.output()['Payload']['status'], value='CREATE IN_PROGRESS'),\n",
    "    next_step=wait_state_solutionversion\n",
    ")\n",
    "\n",
    "solutionversion_choice_state.default_choice(next_step=Fail(\"create_solution_version_failed\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetimportjob_succeed_state = Succeed(\n",
    "    state_id=\"The Solution Version ready?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetimportjob_choice_state = Choice(\n",
    "    state_id=\"Is the DataSet Import Job ready?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetimportjob_choice_state.add_choice(\n",
    "    rule=ChoiceRule.StringEquals(variable=lambda_state_datasetimportjob_status.output()['Payload']['status'], value='ACTIVE'),\n",
    "    next_step=datasetimportjob_succeed_state   \n",
    ")\n",
    "datasetimportjob_choice_state.add_choice(\n",
    "    ChoiceRule.StringEquals(variable=lambda_state_datasetimportjob_status.output()['Payload']['status'], value='CREATE PENDING'),\n",
    "    next_step=wait_state_datasetimportjob\n",
    ")\n",
    "datasetimportjob_choice_state.add_choice(\n",
    "    ChoiceRule.StringEquals(variable=lambda_state_datasetimportjob_status.output()['Payload']['status'], value='CREATE IN_PROGRESS'),\n",
    "    next_step=wait_state_datasetimportjob\n",
    ")\n",
    "\n",
    "\n",
    "datasetimportjob_choice_state.default_choice(next_step=Fail(\"dataset_import_job_failed\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetgroupstatus_choice_state = Choice(\n",
    "    state_id=\"Is the DataSetGroup ready?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Workflow\n",
    "\n",
    "In the following cell, you will define the step that you will use in our workflow.  Then you will create, visualize and execute the workflow. \n",
    "\n",
    "Steps relate to states in AWS Step Functions. For more information, see [States](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-states.html) in the *AWS Step Functions Developer Guide*. For more information on the AWS Step Functions Data Science SDK APIs, see: https://aws-step-functions-data-science-sdk.readthedocs.io. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_workflow_definition=Chain([lambda_state_schema,\n",
    "                                   wait_state_schema,\n",
    "                                   lambda_state_datasetgroup,\n",
    "                                   wait_state_datasetgroup,\n",
    "                                   lambda_state_datasetgroupstatus\n",
    "                                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_workflow = Workflow(\n",
    "    name=\"Dataset-workflow\",\n",
    "    definition=Dataset_workflow_definition,\n",
    "    role=workflow_execution_role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://do0of8uwbahzz.cloudfront.net/graph.css\">\n",
       "<div id=\"graph-337\" class=\"workflowgraph\">\n",
       "    \n",
       "    <svg></svg>\n",
       "    \n",
       "</div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        sfn: \"https://do0of8uwbahzz.cloudfront.net/sfn\",\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sfn'], function(sfn) {\n",
       "    var element = document.getElementById('graph-337')\n",
       "\n",
       "    var options = {\n",
       "        width: parseFloat(getComputedStyle(element, null).width.replace(\"px\", \"\")),\n",
       "        height: 600,\n",
       "        layout: 'LR',\n",
       "        resizeHeight: true\n",
       "    };\n",
       "\n",
       "    var definition = {\"StartAt\": \"create schema\", \"States\": {\"create schema\": {\"Parameters\": {\"FunctionName\": \"stepfunction-create-schema\", \"Payload\": {\"input\": \"personalize-stepfunction-schema263\"}}, \"ResultPath\": \"$\", \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Wait for create schema - 5 secs\", \"Retry\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"IntervalSeconds\": 5, \"MaxAttempts\": 1, \"BackoffRate\": 4.0}], \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"CreateSchemaTaskFailed\"}]}, \"Wait for create schema - 5 secs\": {\"Seconds\": 5, \"Type\": \"Wait\", \"Next\": \"create dataset Group\"}, \"create dataset Group\": {\"Parameters\": {\"FunctionName\": \"stepfunctioncreatedatagroup\", \"Payload\": {\"input\": \"personalize-stepfunction-dataset-group\", \"schemaArn.$\": \"$.Payload.schemaArn\"}}, \"ResultPath\": \"$\", \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Wait for datasetgroup - 30 secs\", \"Retry\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"IntervalSeconds\": 5, \"MaxAttempts\": 1, \"BackoffRate\": 4.0}], \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"CreateDataSetGroupTaskFailed\"}]}, \"Wait for datasetgroup - 30 secs\": {\"Seconds\": 30, \"Type\": \"Wait\", \"Next\": \"check dataset Group status\"}, \"check dataset Group status\": {\"Parameters\": {\"FunctionName\": \"stepfunction_waitforDatasetGroup\", \"Payload\": {\"input.$\": \"$.Payload.datasetGroupArn\", \"schemaArn.$\": \"$.Payload.schemaArn\"}}, \"ResultPath\": \"$\", \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"End\": true, \"Retry\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"IntervalSeconds\": 5, \"MaxAttempts\": 1, \"BackoffRate\": 4.0}], \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"DatasetGroupStatusTaskFailed\"}]}, \"DatasetGroupStatusTaskFailed\": {\"Type\": \"Fail\"}, \"CreateDataSetGroupTaskFailed\": {\"Type\": \"Fail\"}, \"CreateSchemaTaskFailed\": {\"Type\": \"Fail\"}}};\n",
       "    var elementId = '#graph-337';\n",
       "\n",
       "    var graph = new sfn.StateMachineGraph(definition, elementId, options);\n",
       "    graph.render();\n",
       "});\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset_workflow.render_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] Workflow created successfully on AWS Step Functions.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "DatasetWorkflowArn = Dataset_workflow.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetImportWorkflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetImport_workflow_definition=Chain([lambda_state_createdataset,\n",
    "                                   wait_state_dataset,\n",
    "                                   lambda_state_datasetimportjob,\n",
    "                                   wait_state_datasetimportjob,\n",
    "                                   lambda_state_datasetimportjob_status,\n",
    "                                   datasetimportjob_choice_state\n",
    "                                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetImport_workflow = Workflow(\n",
    "    name=\"DatasetImport-workflow\",\n",
    "    definition=DatasetImport_workflow_definition,\n",
    "    role=workflow_execution_role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://do0of8uwbahzz.cloudfront.net/graph.css\">\n",
       "<div id=\"graph-307\" class=\"workflowgraph\">\n",
       "    \n",
       "    <svg></svg>\n",
       "    \n",
       "</div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        sfn: \"https://do0of8uwbahzz.cloudfront.net/sfn\",\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sfn'], function(sfn) {\n",
       "    var element = document.getElementById('graph-307')\n",
       "\n",
       "    var options = {\n",
       "        width: parseFloat(getComputedStyle(element, null).width.replace(\"px\", \"\")),\n",
       "        height: 600,\n",
       "        layout: 'LR',\n",
       "        resizeHeight: true\n",
       "    };\n",
       "\n",
       "    var definition = {\"StartAt\": \"create dataset\", \"States\": {\"create dataset\": {\"Parameters\": {\"FunctionName\": \"stepfunctioncreatedataset\", \"Payload\": {\"schemaArn.$\": \"$.schemaArn\", \"datasetGroupArn.$\": \"$.datasetGroupArn\"}}, \"ResultPath\": \"$\", \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"wait for dataset - 30 secs\", \"Retry\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"IntervalSeconds\": 5, \"MaxAttempts\": 1, \"BackoffRate\": 4.0}], \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"CreateDataSetTaskFailed\"}]}, \"wait for dataset - 30 secs\": {\"Seconds\": 30, \"Type\": \"Wait\", \"Next\": \"create dataset import job\"}, \"create dataset import job\": {\"Parameters\": {\"FunctionName\": \"stepfunction-createdatasetimportjob\", \"Payload\": {\"datasetimportjob\": \"stepfunction-createdatasetimportjob\", \"dataset_arn.$\": \"$.Payload.dataset_arn\", \"datasetGroupArn.$\": \"$.Payload.datasetGroupArn\", \"bucket_name\": \"comprehend-input123\", \"file_name\": \"movie-lens\", \"role_arn\": \"arn:aws:iam::820570838999:role/personalizeaccess\"}}, \"ResultPath\": \"$\", \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Wait for datasetimportjob - 30 secs\", \"Retry\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"IntervalSeconds\": 5, \"MaxAttempts\": 1, \"BackoffRate\": 4.0}], \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"DatasetImportJobTaskFailed\"}]}, \"Wait for datasetimportjob - 30 secs\": {\"Seconds\": 30, \"Type\": \"Wait\", \"Next\": \"check dataset import job status\"}, \"check dataset import job status\": {\"Parameters\": {\"FunctionName\": \"stepfunction_waitfordatasetimportjob\", \"Payload\": {\"dataset_import_job_arn.$\": \"$.Payload.dataset_import_job_arn\", \"datasetGroupArn.$\": \"$.Payload.datasetGroupArn\"}}, \"ResultPath\": \"$\", \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Is the DataSet Import Job ready?\", \"Retry\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"IntervalSeconds\": 5, \"MaxAttempts\": 1, \"BackoffRate\": 4.0}], \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"DatasetImportJobStatusTaskFailed\"}]}, \"Is the DataSet Import Job ready?\": {\"Type\": \"Choice\", \"Choices\": [{\"Variable\": \"$['Payload']['status']\", \"StringEquals\": \"ACTIVE\", \"Next\": \"The Solution Version ready?\"}, {\"Variable\": \"$['Payload']['status']\", \"StringEquals\": \"CREATE PENDING\", \"Next\": \"Wait for datasetimportjob - 30 secs\"}, {\"Variable\": \"$['Payload']['status']\", \"StringEquals\": \"CREATE IN_PROGRESS\", \"Next\": \"Wait for datasetimportjob - 30 secs\"}], \"Default\": \"dataset_import_job_failed\"}, \"dataset_import_job_failed\": {\"Type\": \"Fail\"}, \"The Solution Version ready?\": {\"Type\": \"Succeed\"}, \"DatasetImportJobStatusTaskFailed\": {\"Type\": \"Fail\"}, \"DatasetImportJobTaskFailed\": {\"Type\": \"Fail\"}, \"CreateDataSetTaskFailed\": {\"Type\": \"Fail\"}}};\n",
       "    var elementId = '#graph-307';\n",
       "\n",
       "    var graph = new sfn.StateMachineGraph(definition, elementId, options);\n",
       "    graph.render();\n",
       "});\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DatasetImport_workflow.render_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] Workflow created successfully on AWS Step Functions.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "DatasetImportflowArn = DatasetImport_workflow.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recepie and Solution workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Create_receipe_sol_workflow_definition=Chain([lambda_state_select_receipe_create_solution,\n",
    "                                   wait_state_receipe,\n",
    "                                   lambda_create_solution_version,\n",
    "                                   wait_state_solutionversion,\n",
    "                                   lambda_state_solutionversion_status,\n",
    "                                   solutionversion_choice_state\n",
    "                                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Create_receipe_sol_workflow = Workflow(\n",
    "    name=\"Create_receipe_sol-workflow\",\n",
    "    definition=Create_receipe_sol_workflow_definition,\n",
    "    role=workflow_execution_role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://do0of8uwbahzz.cloudfront.net/graph.css\">\n",
       "<div id=\"graph-301\" class=\"workflowgraph\">\n",
       "    \n",
       "    <svg></svg>\n",
       "    \n",
       "</div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        sfn: \"https://do0of8uwbahzz.cloudfront.net/sfn\",\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sfn'], function(sfn) {\n",
       "    var element = document.getElementById('graph-301')\n",
       "\n",
       "    var options = {\n",
       "        width: parseFloat(getComputedStyle(element, null).width.replace(\"px\", \"\")),\n",
       "        height: 600,\n",
       "        layout: 'LR',\n",
       "        resizeHeight: true\n",
       "    };\n",
       "\n",
       "    var definition = {\"StartAt\": \"select receipe and create solution\", \"States\": {\"select receipe and create solution\": {\"Parameters\": {\"FunctionName\": \"stepfunction_select-recipe_create-solution\", \"Payload\": {\"dataset_group_arn.$\": \"$.datasetGroupArn\"}}, \"ResultPath\": \"$\", \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Wait for receipe - 30 secs\", \"Retry\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"IntervalSeconds\": 5, \"MaxAttempts\": 1, \"BackoffRate\": 4.0}], \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"DatasetReceiptCreateSolutionTaskFailed\"}]}, \"Wait for receipe - 30 secs\": {\"Seconds\": 30, \"Type\": \"Wait\", \"Next\": \"create solution version\"}, \"create solution version\": {\"Parameters\": {\"FunctionName\": \"stepfunction_create_solution_version\", \"Payload\": {\"solution_arn.$\": \"$.Payload.solution_arn\"}}, \"ResultPath\": \"$\", \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Wait for solution version - 60 secs\", \"Retry\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"IntervalSeconds\": 5, \"MaxAttempts\": 1, \"BackoffRate\": 4.0}], \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"CreateSolutionVersionTaskFailed\"}]}, \"Wait for solution version - 60 secs\": {\"Seconds\": 60, \"Type\": \"Wait\", \"Next\": \"check solution version status\"}, \"check solution version status\": {\"Parameters\": {\"FunctionName\": \"stepfunction_waitforSolutionVersion\", \"Payload\": {\"solution_version_arn.$\": \"$.Payload.solution_version_arn\"}}, \"ResultPath\": \"$\", \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Is the Solution Version ready?\", \"Retry\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"IntervalSeconds\": 5, \"MaxAttempts\": 1, \"BackoffRate\": 4.0}], \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"SolutionVersionStatusTaskFailed\"}]}, \"Is the Solution Version ready?\": {\"Type\": \"Choice\", \"Choices\": [{\"Variable\": \"$['Payload']['status']\", \"StringEquals\": \"ACTIVE\", \"Next\": \"The Solution Version ready?\"}, {\"Variable\": \"$['Payload']['status']\", \"StringEquals\": \"CREATE PENDING\", \"Next\": \"Wait for solution version - 60 secs\"}, {\"Variable\": \"$['Payload']['status']\", \"StringEquals\": \"CREATE IN_PROGRESS\", \"Next\": \"Wait for solution version - 60 secs\"}], \"Default\": \"create_solution_version_failed\"}, \"create_solution_version_failed\": {\"Type\": \"Fail\"}, \"The Solution Version ready?\": {\"Type\": \"Succeed\"}, \"SolutionVersionStatusTaskFailed\": {\"Type\": \"Fail\"}, \"CreateSolutionVersionTaskFailed\": {\"Type\": \"Fail\"}, \"DatasetReceiptCreateSolutionTaskFailed\": {\"Type\": \"Fail\"}}};\n",
       "    var elementId = '#graph-301';\n",
       "\n",
       "    var graph = new sfn.StateMachineGraph(definition, elementId, options);\n",
       "    graph.render();\n",
       "});\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Create_receipe_sol_workflow.render_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] Workflow created successfully on AWS Step Functions.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "CreateReceipeArn = Create_receipe_sol_workflow.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Campaign Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Create_Campaign_workflow_definition=Chain([lambda_create_campaign,\n",
    "                                   wait_state_campaign,\n",
    "                                   lambda_state_campaign_status,\n",
    "                                   wait_state_datasetimportjob,\n",
    "                                   create_campaign_choice_state\n",
    "                                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Campaign_workflow = Workflow(\n",
    "    name=\"Campaign-workflow\",\n",
    "    definition=Create_Campaign_workflow_definition,\n",
    "    role=workflow_execution_role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://do0of8uwbahzz.cloudfront.net/graph.css\">\n",
       "<div id=\"graph-206\" class=\"workflowgraph\">\n",
       "    \n",
       "    <svg></svg>\n",
       "    \n",
       "</div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        sfn: \"https://do0of8uwbahzz.cloudfront.net/sfn\",\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sfn'], function(sfn) {\n",
       "    var element = document.getElementById('graph-206')\n",
       "\n",
       "    var options = {\n",
       "        width: parseFloat(getComputedStyle(element, null).width.replace(\"px\", \"\")),\n",
       "        height: 600,\n",
       "        layout: 'LR',\n",
       "        resizeHeight: true\n",
       "    };\n",
       "\n",
       "    var definition = {\"StartAt\": \"create campaign\", \"States\": {\"create campaign\": {\"Parameters\": {\"FunctionName\": \"stepfunction_getsolution_metric_create_campaign\", \"Payload\": {\"solution_version_arn.$\": \"$.solution_version_arn\"}}, \"ResultPath\": \"$\", \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Wait for Campaign - 30 secs\", \"Retry\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"IntervalSeconds\": 5, \"MaxAttempts\": 1, \"BackoffRate\": 4.0}], \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"CreateCampaignTaskFailed\"}]}, \"Wait for Campaign - 30 secs\": {\"Seconds\": 30, \"Type\": \"Wait\", \"Next\": \"check campaign status\"}, \"check campaign status\": {\"Parameters\": {\"FunctionName\": \"stepfunction_waitforCampaign\", \"Payload\": {\"campaign_arn.$\": \"$.Payload.campaign_arn\"}}, \"ResultPath\": \"$\", \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Wait for datasetimportjob - 30 secs\", \"Retry\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"IntervalSeconds\": 5, \"MaxAttempts\": 1, \"BackoffRate\": 4.0}], \"Catch\": [{\"ErrorEquals\": [\"States.TaskFailed\"], \"Next\": \"CampaignStatusTaskFailed\"}]}, \"Wait for datasetimportjob - 30 secs\": {\"Seconds\": 30, \"Type\": \"Wait\", \"Next\": \"Is the Campaign ready?\"}, \"Is the Campaign ready?\": {\"Type\": \"Choice\", \"Choices\": [{\"Variable\": \"$['Payload']['status']\", \"StringEquals\": \"ACTIVE\", \"Next\": \"CampaignCreatedSuccessfully\"}, {\"Variable\": \"$['Payload']['status']\", \"StringEquals\": \"CREATE PENDING\", \"Next\": \"Wait for Campaign - 30 secs\"}, {\"Variable\": \"$['Payload']['status']\", \"StringEquals\": \"CREATE IN_PROGRESS\", \"Next\": \"Wait for Campaign - 30 secs\"}], \"Default\": \"CreateCampaignFailed\"}, \"CreateCampaignFailed\": {\"Type\": \"Fail\"}, \"CampaignCreatedSuccessfully\": {\"Type\": \"Succeed\"}, \"CampaignStatusTaskFailed\": {\"Type\": \"Fail\"}, \"CreateCampaignTaskFailed\": {\"Type\": \"Fail\"}}};\n",
       "    var elementId = '#graph-206';\n",
       "\n",
       "    var graph = new sfn.StateMachineGraph(definition, elementId, options);\n",
       "    graph.render();\n",
       "});\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Campaign_workflow.render_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] Workflow created successfully on AWS Step Functions.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "CreateCampaignArn = Campaign_workflow.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_dataset_workflow_state = Task(\n",
    "    state_id=\"DataSetWorkflow\",\n",
    "    resource=\"arn:aws:states:::states:startExecution.sync:2\",\n",
    "    parameters={\n",
    "                                \"Input\": \"true\",\n",
    "                                #\"StateMachineArn\": \"arn:aws:states:us-east-1:444602785259:stateMachine:Dataset-workflow\",\n",
    "                                \"StateMachineArn\": DatasetWorkflowArn\n",
    "                }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_datasetImport_workflow_state = Task(\n",
    "    state_id=\"DataSetImportWorkflow\",\n",
    "    resource=\"arn:aws:states:::states:startExecution.sync:2\",\n",
    "    parameters={\n",
    "                                 \"Input\":{\n",
    "                                    \"schemaArn.$\": \"$.Output.Payload.schemaArn\",\n",
    "                                    \"datasetGroupArn.$\": \"$.Output.Payload.datasetGroupArn\"\n",
    "                                   },\n",
    "                                \"StateMachineArn\": DatasetImportflowArn,\n",
    "                }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_receipe_solution_workflow_state = Task(\n",
    "    state_id=\"ReceipeSolutionWorkflow\",\n",
    "    resource=\"arn:aws:states:::states:startExecution.sync:2\",\n",
    "    parameters={\n",
    "                                 \"Input\":{\n",
    "                                    \"datasetGroupArn.$\": \"$.Output.Payload.datasetGroupArn\"\n",
    "\n",
    "                                   },\n",
    "                                \"StateMachineArn\": CreateReceipeArn\n",
    "                }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_campaign_solution_workflow_state = Task(\n",
    "    state_id=\"CampaignWorkflow\",\n",
    "    resource=\"arn:aws:states:::states:startExecution.sync:2\",\n",
    "    parameters={\n",
    "                                 \"Input\":{\n",
    "                                    \"solution_version_arn.$\": \"$.Output.Payload.solution_version_arn\"\n",
    "\n",
    "                                   },\n",
    "                                \"StateMachineArn\": CreateCampaignArn\n",
    "                }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Main_workflow_definition=Chain([call_dataset_workflow_state,\n",
    "                                call_datasetImport_workflow_state,\n",
    "                                call_receipe_solution_workflow_state,\n",
    "                                call_campaign_solution_workflow_state\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Main_workflow = Workflow(\n",
    "    name=\"Main-workflow\",\n",
    "    definition=Main_workflow_definition,\n",
    "    role=workflow_execution_role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://do0of8uwbahzz.cloudfront.net/graph.css\">\n",
       "<div id=\"graph-404\" class=\"workflowgraph\">\n",
       "    \n",
       "    <svg></svg>\n",
       "    \n",
       "</div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        sfn: \"https://do0of8uwbahzz.cloudfront.net/sfn\",\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sfn'], function(sfn) {\n",
       "    var element = document.getElementById('graph-404')\n",
       "\n",
       "    var options = {\n",
       "        width: parseFloat(getComputedStyle(element, null).width.replace(\"px\", \"\")),\n",
       "        height: 600,\n",
       "        layout: 'LR',\n",
       "        resizeHeight: true\n",
       "    };\n",
       "\n",
       "    var definition = {\"StartAt\": \"DataSetWorkflow\", \"States\": {\"DataSetWorkflow\": {\"Resource\": \"arn:aws:states:::states:startExecution.sync:2\", \"Parameters\": {\"Input\": \"true\", \"StateMachineArn\": \"arn:aws:states:us-east-1:820570838999:stateMachine:Dataset-workflow\"}, \"Type\": \"Task\", \"Next\": \"DataSetImportWorkflow\"}, \"DataSetImportWorkflow\": {\"Resource\": \"arn:aws:states:::states:startExecution.sync:2\", \"Parameters\": {\"Input\": {\"schemaArn.$\": \"$.Output.Payload.schemaArn\", \"datasetGroupArn.$\": \"$.Output.Payload.datasetGroupArn\"}, \"StateMachineArn\": \"arn:aws:states:us-east-1:820570838999:stateMachine:DatasetImport-workflow\"}, \"Type\": \"Task\", \"Next\": \"ReceipeSolutionWorkflow\"}, \"ReceipeSolutionWorkflow\": {\"Resource\": \"arn:aws:states:::states:startExecution.sync:2\", \"Parameters\": {\"Input\": {\"datasetGroupArn.$\": \"$.Output.Payload.datasetGroupArn\"}, \"StateMachineArn\": \"arn:aws:states:us-east-1:820570838999:stateMachine:Create_receipe_sol-workflow\"}, \"Type\": \"Task\", \"Next\": \"CampaignWorkflow\"}, \"CampaignWorkflow\": {\"Resource\": \"arn:aws:states:::states:startExecution.sync:2\", \"Parameters\": {\"Input\": {\"solution_version_arn.$\": \"$.Output.Payload.solution_version_arn\"}, \"StateMachineArn\": \"arn:aws:states:us-east-1:820570838999:stateMachine:Campaign-workflow\"}, \"Type\": \"Task\", \"End\": true}}};\n",
       "    var elementId = '#graph-404';\n",
       "\n",
       "    var graph = new sfn.StateMachineGraph(definition, elementId, options);\n",
       "    graph.render();\n",
       "});\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Main_workflow.render_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] Workflow created successfully on AWS Step Functions.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'arn:aws:states:us-east-1:820570838999:stateMachine:Main-workflow'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Main_workflow.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] Workflow execution started successfully on AWS Step Functions.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "Main_workflow_execution = Main_workflow.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main_workflow_execution = Workflow(\n",
    "    name=\"Campaign_Workflow\",\n",
    "    definition=path1,\n",
    "    role=workflow_execution_role\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main_workflow_execution.render_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and execute the workflow\n",
    "\n",
    "In the next cells, we will create the branching happy workflow in AWS Step Functions with [create](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.create) and execute it with [execute](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.execute).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#personalize_workflow.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#personalize_workflow_execution = happy_workflow.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Review the workflow progress\n",
    "\n",
    "Review the workflow progress with the [render_progress](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.render_progress).\n",
    "\n",
    "Review the execution history by calling [list_events](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.list_events) to list all events in the workflow execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://do0of8uwbahzz.cloudfront.net/graph.css\">\n",
       "<div id=\"graph-661\" class=\"workflowgraph\">\n",
       "    \n",
       "    <style>\n",
       "        .graph-legend ul {\n",
       "            list-style-type: none;\n",
       "            padding: 10px;\n",
       "            padding-left: 0;\n",
       "            margin: 0;\n",
       "            position: absolute;\n",
       "            top: 0;\n",
       "            background: transparent;\n",
       "        }\n",
       "\n",
       "        .graph-legend li {\n",
       "            margin-left: 10px;\n",
       "            display: inline-block;\n",
       "        }\n",
       "\n",
       "        .graph-legend li > div {\n",
       "            width: 10px;\n",
       "            height: 10px;\n",
       "            display: inline-block;\n",
       "        }\n",
       "\n",
       "        .graph-legend .success { background-color: #2BD62E }\n",
       "        .graph-legend .failed { background-color: #DE322F }\n",
       "        .graph-legend .cancelled { background-color: #DDDDDD }\n",
       "        .graph-legend .in-progress { background-color: #53C9ED }\n",
       "        .graph-legend .caught-error { background-color: #FFA500 }\n",
       "    </style>\n",
       "    <div class=\"graph-legend\">\n",
       "        <ul>\n",
       "            <li>\n",
       "                <div class=\"success\"></div>\n",
       "                <span>Success</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"failed\"></div>\n",
       "                <span>Failed</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"cancelled\"></div>\n",
       "                <span>Cancelled</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"in-progress\"></div>\n",
       "                <span>In Progress</span>\n",
       "            </li>\n",
       "            <li>\n",
       "                <div class=\"caught-error\"></div>\n",
       "                <span>Caught Error</span>\n",
       "            </li>\n",
       "        </ul>\n",
       "    </div>\n",
       "\n",
       "    <svg></svg>\n",
       "    <a href=\"https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:820570838999:execution:Main-workflow:862cad9c-7b73-42c9-97a2-17d194e047c0\" target=\"_blank\"> Inspect in AWS Step Functions </a>\n",
       "</div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        sfn: \"https://do0of8uwbahzz.cloudfront.net/sfn\",\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sfn'], function(sfn) {\n",
       "    var element = document.getElementById('graph-661')\n",
       "\n",
       "    var options = {\n",
       "        width: parseFloat(getComputedStyle(element, null).width.replace(\"px\", \"\")),\n",
       "        height: 1000,\n",
       "        layout: 'LR',\n",
       "        resizeHeight: true\n",
       "    };\n",
       "\n",
       "    var definition = {\"StartAt\": \"DataSetWorkflow\", \"States\": {\"DataSetWorkflow\": {\"Resource\": \"arn:aws:states:::states:startExecution.sync:2\", \"Parameters\": {\"Input\": \"true\", \"StateMachineArn\": \"arn:aws:states:us-east-1:820570838999:stateMachine:Dataset-workflow\"}, \"Type\": \"Task\", \"Next\": \"DataSetImportWorkflow\"}, \"DataSetImportWorkflow\": {\"Resource\": \"arn:aws:states:::states:startExecution.sync:2\", \"Parameters\": {\"Input\": {\"schemaArn.$\": \"$.Output.Payload.schemaArn\", \"datasetGroupArn.$\": \"$.Output.Payload.datasetGroupArn\"}, \"StateMachineArn\": \"arn:aws:states:us-east-1:820570838999:stateMachine:DatasetImport-workflow\"}, \"Type\": \"Task\", \"Next\": \"ReceipeSolutionWorkflow\"}, \"ReceipeSolutionWorkflow\": {\"Resource\": \"arn:aws:states:::states:startExecution.sync:2\", \"Parameters\": {\"Input\": {\"datasetGroupArn.$\": \"$.Output.Payload.datasetGroupArn\"}, \"StateMachineArn\": \"arn:aws:states:us-east-1:820570838999:stateMachine:Create_receipe_sol-workflow\"}, \"Type\": \"Task\", \"Next\": \"CampaignWorkflow\"}, \"CampaignWorkflow\": {\"Resource\": \"arn:aws:states:::states:startExecution.sync:2\", \"Parameters\": {\"Input\": {\"solution_version_arn.$\": \"$.Output.Payload.solution_version_arn\"}, \"StateMachineArn\": \"arn:aws:states:us-east-1:820570838999:stateMachine:Campaign-workflow\"}, \"Type\": \"Task\", \"End\": true}}};\n",
       "    var elementId = '#graph-661';\n",
       "    var events = { 'events': [{\"timestamp\": 1607976819.929, \"type\": \"ExecutionStarted\", \"id\": 1, \"previousEventId\": 0, \"executionStartedEventDetails\": {\"input\": \"{}\", \"inputDetails\": {\"truncated\": false}, \"roleArn\": \"arn:aws:iam::820570838999:role/StepFunctionsWorkflowExecutionRole\"}}, {\"timestamp\": 1607976819.956, \"type\": \"TaskStateEntered\", \"id\": 2, \"previousEventId\": 0, \"stateEnteredEventDetails\": {\"name\": \"DataSetWorkflow\", \"input\": \"{}\", \"inputDetails\": {\"truncated\": false}}}, {\"timestamp\": 1607976819.956, \"type\": \"TaskScheduled\", \"id\": 3, \"previousEventId\": 2, \"taskScheduledEventDetails\": {\"resourceType\": \"states\", \"resource\": \"startExecution.sync:2\", \"region\": \"us-east-1\", \"parameters\": \"{\\\"Input\\\":\\\"true\\\",\\\"StateMachineArn\\\":\\\"arn:aws:states:us-east-1:820570838999:stateMachine:Dataset-workflow\\\"}\"}}, {\"timestamp\": 1607976819.987, \"type\": \"TaskStarted\", \"id\": 4, \"previousEventId\": 3, \"taskStartedEventDetails\": {\"resourceType\": \"states\", \"resource\": \"startExecution.sync:2\"}}, {\"timestamp\": 1607976820.025, \"type\": \"TaskSubmitFailed\", \"id\": 5, \"previousEventId\": 4, \"taskSubmitFailedEventDetails\": {\"resourceType\": \"states\", \"resource\": \"startExecution.sync:2\", \"error\": \"StepFunctions.AWSStepFunctionsException\", \"cause\": \"User: arn:aws:sts::820570838999:assumed-role/StepFunctionsWorkflowExecutionRole/DFDJssKNXuDSBlWtBaGGOBecQDiogYhZ is not authorized to perform: states:StartExecution on resource: arn:aws:states:us-east-1:820570838999:stateMachine:Dataset-workflow (Service: AWSStepFunctions; Status Code: 400; Error Code: AccessDeniedException; Request ID: 5f232125-568f-4315-bfc7-5c4091b85fe6; Proxy: null)\"}}, {\"timestamp\": 1607976820.025, \"type\": \"ExecutionFailed\", \"id\": 6, \"previousEventId\": 5, \"executionFailedEventDetails\": {\"error\": \"StepFunctions.AWSStepFunctionsException\", \"cause\": \"User: arn:aws:sts::820570838999:assumed-role/StepFunctionsWorkflowExecutionRole/DFDJssKNXuDSBlWtBaGGOBecQDiogYhZ is not authorized to perform: states:StartExecution on resource: arn:aws:states:us-east-1:820570838999:stateMachine:Dataset-workflow (Service: AWSStepFunctions; Status Code: 400; Error Code: AccessDeniedException; Request ID: 5f232125-568f-4315-bfc7-5c4091b85fe6; Proxy: null)\"}}] };\n",
       "\n",
       "    var graph = new sfn.StateMachineExecutionGraph(definition, events, elementId, options);\n",
       "    graph.render();\n",
       "});\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Main_workflow_execution.render_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not 'bool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-c37e53cb873f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMain_workflow_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/stepfunctions/workflow/stepfunctions.py\u001b[0m in \u001b[0;36mlist_events\u001b[0;34m(self, max_items, reverse_order, html)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mevents_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/stepfunctions/workflow/stepfunctions.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mEventsTableWidget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/stepfunctions/workflow/widgets/events_table.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, events)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mtimestamp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mevent_detail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_event_detail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         ) for event in events]\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTABLE_TEMPLATE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/stepfunctions/workflow/widgets/events_table.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mtimestamp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mevent_detail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_event_detail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         ) for event in events]\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTABLE_TEMPLATE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/stepfunctions/workflow/widgets/events_table.py\u001b[0m in \u001b[0;36m_format_event_detail\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_format_event_detail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mevent_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_step_detail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unpack_to_proper_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_details\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_details\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/stepfunctions/workflow/widgets/events_table.py\u001b[0m in \u001b[0;36m_unpack_to_proper_dict\u001b[0;34m(self, dictionary)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unpack_to_proper_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/stepfunctions/workflow/widgets/events_table.py\u001b[0m in \u001b[0;36m_unpack_to_proper_dict\u001b[0;34m(self, dictionary)\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unpack_to_proper_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/stepfunctions/workflow/widgets/events_table.py\u001b[0m in \u001b[0;36m_load_json\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             raise TypeError('the JSON object must be str, bytes or bytearray, '\n\u001b[0;32m--> 348\u001b[0;31m                             'not {!r}'.format(s.__class__.__name__))\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surrogatepass'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not 'bool'"
     ]
    }
   ],
   "source": [
    "Main_workflow_execution.list_events(html=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate-Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have a successful campaign, let's generate recommendations for the campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a User and an Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('./ml-100k/u.item', sep='|', usecols=[0,1], encoding='latin-1')\n",
    "items.columns = ['ITEM_ID', 'TITLE']\n",
    "\n",
    "\n",
    "user_id, item_id, rating, timestamp = data.sample().values[0]\n",
    "\n",
    "user_id = int(user_id)\n",
    "item_id = int(item_id)\n",
    "\n",
    "print(\"user_id\",user_id)\n",
    "print(\"items\",items)\n",
    "\n",
    "\n",
    "item_title = items.loc[items['ITEM_ID'] == item_id].values[0][-1]\n",
    "print(\"USER: {}\".format(user_id))\n",
    "print(\"ITEM: {}\".format(item_title))\n",
    "print(\"ITEM ID: {}\".format(item_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_recommendations = Wait(\n",
    "    state_id=\"Wait for recommendations - 10 secs\",\n",
    "    seconds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lambda Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_state_get_recommendations = LambdaStep(\n",
    "    state_id=\"get recommendations\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"stepfunction_getRecommendations\", \n",
    "        \"Payload\": {  \n",
    "           \"campaign_arn\": 'arn:aws:personalize:us-east-1:261602857181:campaign/stepfunction-campaign',            \n",
    "           \"user_id\": user_id,  \n",
    "           \"item_id\": item_id             \n",
    "        }\n",
    "    },\n",
    "    result_path = '$'\n",
    ")\n",
    "\n",
    "lambda_state_get_recommendations.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=5,\n",
    "    max_attempts=1,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "lambda_state_get_recommendations.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"GetRecommendationTaskFailed\")\n",
    "    #next_step=recommendation_path   \n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Succeed State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_complete = Succeed(\"WorkflowComplete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_path = Chain([ \n",
    "lambda_state_get_recommendations,\n",
    "wait_recommendations,\n",
    "workflow_complete\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define, Create, Render, and Execute Recommendation Workflow\n",
    "\n",
    "In the next cells, we will create a workflow in AWS Step Functions with [create](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.create) and execute it with [execute](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.execute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_workflow = Workflow(\n",
    "    name=\"Recommendation_Workflow4\",\n",
    "    definition=recommendation_path,\n",
    "    role=workflow_execution_role\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_workflow.render_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_workflow.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_workflow_execution = recommendation_workflow.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review progress\n",
    "\n",
    "Review workflow progress with the [render_progress](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.render_progress).\n",
    "\n",
    "Review execution history by calling [list_events](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.list_events) to list all events in the workflow execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_workflow_execution.render_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recommendation_workflow_execution.list_events(html=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list = recommendation_workflow_execution.get_output()['Payload']['item_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list = recommendation_workflow_execution.get_output()['Payload']['item_list']\n",
    "\n",
    "print(\"Recommendations:\")\n",
    "for item in item_list:\n",
    "    np.int(item['itemId'])\n",
    "    item_title = items.loc[items['ITEM_ID'] == np.int(item['itemId'])].values[0][-1]\n",
    "    print(item_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up Amazon Personalize resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to clean up the Amazon Personalize and the state machines created blog. Login to Amazon Personalize console and delete resources such as Dataset Groups, Dataset, Solutions, Receipts, and Campaign. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up State Machine resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Campaign_workflow.delete()\n",
    "\n",
    "recommendation_workflow.delete()\n",
    "\n",
    "Main_workflow.delete()\n",
    "\n",
    "Create_receipe_sol_workflow.delete()\n",
    "\n",
    "DatasetImport_workflow.delete()\n",
    "\n",
    "Dataset_workflow.delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
